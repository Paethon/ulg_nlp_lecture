{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ce7aff8-26d1-4181-942a-d60a484d9387",
   "metadata": {},
   "source": [
    "## Set Up Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d1a354c-d1c8-4a78-a45a-4ebce6dce9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "597521cc-7c3b-4834-bfaf-97ff68ad9127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4cbcbc-27ab-4904-95cb-9b08a9795214",
   "metadata": {},
   "source": [
    "## Load Tokenizer and Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ddd8f8dc-e9fc-4db2-9ae6-b396710ca087",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = transformers.AutoModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd978b5-9ebe-4318-a7f3-9c997bc60577",
   "metadata": {},
   "source": [
    "## Do Some Tokenization Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "97c1aa22-edf3-4f4a-9b92-1e801c5de394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'this', 'is', 'a', 'test', '!', '[SEP]']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"This is a test!\", add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d4173ff2-cfee-428b-aa6b-b3ddab5ca111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'we',\n",
       " 'are',\n",
       " 'practicing',\n",
       " 'token',\n",
       " '##ization',\n",
       " 'some',\n",
       " 'more',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"We are practicing tokenization some more\", add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1f715393-1794-432f-bc93-470800623362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2023, 2003, 1037, 3231, 999, 102]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"This is a test!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfccafb2-d020-498c-80e5-78a35d4d94bc",
   "metadata": {},
   "source": [
    "## Token Embeddings have Some Interesting Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "07c0b7c4-2ebc-4b83-9ad9-4c578137f5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(word):\n",
    "    input_ids = tokenizer.encode(word, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    assert input_ids.shape[1] == 3\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(input_ids, output_hidden_states=True).hidden_states\n",
    "    return last_hidden_states[0][0,1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b0813-9e27-4b40-9497-bcc4e1ae9dc2",
   "metadata": {},
   "source": [
    "Cosine similarity can be used to measure the similarity between two vecors (cos of the angle between the vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "083af505-46bd-4421-af0e-03482ea01acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(word1, word2):\n",
    "    w1_emb = get_embedding(word1)\n",
    "    w2_emb = get_embedding(word2)\n",
    "    return torch.cosine_similarity(w1_emb, w2_emb, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ecf775ff-cae3-4573-abbf-526b3f5f5483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6223)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare(\"king\", \"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "79366dd9-abf2-43c7-b5eb-c9fd2c44099e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2913)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare(\"king\", \"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9989e439-5eb2-43d9-aa96-e399a479c435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2266)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare(\"king\", \"woman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f211eb25-c961-482d-8fa3-0f46531f532c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1750)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare(\"queen\", \"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0d4e8920-eee2-42c3-acde-7e1414a85ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3524)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare(\"queen\", \"woman\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
